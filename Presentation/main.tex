\documentclass{beamer}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{graphicx}
% \usetheme{Madrid}
% \usecolortheme{seahorse}
\usetheme{CambridgeUS}
\usecolortheme{crane}

\graphicspath{{./a/}}

\title{Unsupervised Domain Adaptation}
\author{Kintan Saha \hspace{} Pratham Gupta \\
    Gavish Bansal \hspace{} Krishna Agarwal }
\institute{Indian Institute of Science, Bangalore}
\date{\today}

\begin{document}

% Title Slide
\frame{\titlepage}

\begin{frame}
    \frametitle{Project Motivation}
    Our goal is to explore the field of \textbf{Unsupervised Domain Adaptation (UDA)} and implement some of the state-of-the-art methods.\\
    Flow of the presentation:
    \begin{itemize}
        \item Introduction to UDA
        \item Domain-Adversarial Training of Neural Networks (DANN)
        \item Survey of Other Methods of UDA
        \begin{itemize}
            \item Divergence Based Methods
            \item Reconstruction Based Methods
            \item Ensemble Methods
        \end{itemize}
    \end{itemize}    

\end{frame}

\begin{frame}
    \frametitle{What is UDA?}
    \textbf{Domain Adaptation (DA)} is a subfield of machine learning that focuses on transferring knowledge from a source domain to a target domain with different data distributions.\\
    \textbf{Key Idea:} The goal is to learn a model that performs well on the target domain, even when only labeled data from the source domain is available.\\
    \textbf{Challenges:} The main challenge in UDA is the domain shift, which occurs when the source and target domains have different distributions. This can lead to poor performance of standard learning techniques.\\
    \begin{figure}
        \centering
        \includegraphics[width=0.5\linewidth]{Example of Domain Shift.png}
        \caption{Example of domain shift between source and target domains}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{What is UDA?}
    In \textbf{Unsupervised Domain Adaptation (UDA)}, only the source domain data is labeled, while the target domain data is unlabeled.\\

    \textbf{Applications:} 
    \begin{itemize}
        \item When shifting a model trained on \textbf{synthetic data} to \textbf{real-world data}, the model may not perform well due to the differences in data distributions.
        \item For systems like \textbf{Security Cameras} and \textbf{Autonomous Driving}, the inputs to each camera belong to different domains. So we wish to adapt the model to the new domain.
    \end{itemize}

    

\end{frame}
% % Slide 1 - Motivation
% \begin{frame}{Project Motivation}

% \begin{itemize}
%     \item \textbf{Domain adaptation} aims to transfer knowledge from a source domain to a target domain with different data distributions.
%     \item In the \textbf{unsupervised} setting, only source domain data is labeled; target domain data is unlabeled.
%     \item Standard learning techniques fail due to the \textbf{domain shift}.
%     \item To address this, we train models to learn \textbf{domain-invariant features} that perform well across domains.
% \end{itemize}
% \end{frame}

% % Slide 2 - Setup
% \begin{frame}{Domain Adaptation Setup}
% \begin{itemize}
%     \item Input space: $\mathcal{X}$, Label space: $\mathcal{Y} = \{0, 1, \ldots, L-1\}$
%     \item \textbf{Source distribution:} $\mathcal{D}_S$ over $\mathcal{X} \times \mathcal{Y}$
%     \item \textbf{Target distribution:} $\mathcal{D}_T$ over $\mathcal{X} \times \mathcal{Y}$
%     \item Training data:
%     \begin{itemize}
%         \item Labeled source set: $S = \{(x_i, y_i)\}_{i=1}^{n}$
%         \item Unlabeled target set: $T = \{x_j\}_{j=1}^{n'}$
%     \end{itemize}
% \end{itemize}
% \end{frame}

% Slide 3 - H-Divergence
\begin{frame}{$\mathcal{H}$-Divergence}
\[
d_{\mathcal{H}}(\mathcal{D}_S^X, \mathcal{D}_T^X) = 2 \sup_{\eta \in \mathcal{H}} \left| \Pr_{x \sim \mathcal{D}_S^X}[\eta(x)=1] - \Pr_{x \sim \mathcal{D}_T^X}[\eta(x)=1] \right|
\]

\begin{itemize}
    \item Measures the ability of a hypothesis $\eta \in \mathcal{H}$ to distinguish between source and target distributions.
    \item High $d_{\mathcal{H}}$ implies easy domain discrimination — bad for transfer.
    \item For symmetric hypothesis classes, it can be estimated via:
    \[
    \hat{d}_A = 2 (1 - 2\epsilon)
    \]
    \item Here, $\epsilon$ is the generalization error of a domain classifier.
\end{itemize}
\end{frame}

% Slide 4 - DANN Architecture
\begin{frame}{DANN Architecture}
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{DANN_img.png}
    \caption{Domain-Adversarial Neural Network (DANN) Architecture}
\end{figure}
\end{frame}



% Slide 5 - Key Idea
\begin{frame}{DANN: Key Idea}
\begin{itemize}
    \item Simultaneous optimization of:
    \begin{enumerate}
        \item \textbf{Label predictor} — classifies source data correctly
        \item \textbf{Domain classifier} — fails to distinguish source from target
    \end{enumerate}
    \item The \textbf{feature extractor} is trained to:
    \begin{itemize}
        \item Minimize label prediction loss (task objective)
        \item Maximize domain classification loss (encourages invariance)
    \end{itemize}
\end{itemize}
\end{frame}

% Slide 6 - Architecture Overview
\begin{frame}{DANN Architecture Overview}
\begin{itemize}
    \item \textbf{Feature extractor} $G_f$: shared across tasks
    \item \textbf{Label predictor} $G_y$: trained on labeled source data
    \item \textbf{Domain classifier} $G_d$: trained to classify domain
    \item \textbf{Gradient Reversal Layer (GRL)}: connects $G_f$ and $G_d$
\end{itemize}
\end{frame}

% Slide 7 - GRL
\begin{frame}{Gradient Reversal Layer (GRL)}
\begin{itemize}
    \item \textbf{Forward pass:} identity function: $R(x) = x$
    \item \textbf{Backward pass:} gradient scaled by $-\lambda$: 
    \[
    \frac{dR}{dx} = -\lambda I
    \]
    \item GRL enables adversarial training by reversing gradients from the domain classifier.
    \item This pushes $G_f$ to generate domain-invariant features.
\end{itemize}
\end{frame}

% Slide 8 - Objective Function
\begin{frame}{Objective Function}
\[
\min_{\theta_f, \theta_y} \max_{\theta_d} \mathcal{E}(\theta_f, \theta_y, \theta_d)
\]
\vspace{-1em}
\[
\mathcal{E} = \frac{1}{n} \sum_{i=1}^{n} L_y(G_y(G_f(x_i)); y_i) 
- \lambda \sum_{j=1}^{n+n'} L_d(G_d(G_f(x_j)); d_j)
\]
\begin{itemize}
    \item \textbf{Label loss:} minimized w.r.t. $\theta_f$, $\theta_y$
    \item \textbf{Domain loss:} maximized w.r.t. $\theta_f$, minimized w.r.t. $\theta_d$
    \item $\lambda$ balances the two objectives
\end{itemize}
\end{frame}

\begin{frame}{Experiments}
The experiments section is divided into 3 parts: 
\begin{itemize}
    \item Experiments on shallow DNN
    \item Experiments on image classification
    \item Experiments on image reconstruction
\end{itemize}
    
\end{frame}


\begin{frame}{Experiments on Shallow DNN}
\vspace{-0.2cm}

\textbf{Intertwining moons}: Dataset used for evaluating domain adaptation.\\[0.2cm]
The structure of source and target distributions is shown below:

\vspace{0.5cm}

\begin{center}
    \includegraphics[width=0.7\linewidth]{moon.png}
    \captionof{figure}{\small\textbf{Source and Target Distributions}. \textcolor{red}{Red} and \textcolor{green!70!black}{green} points represent source data (Class 0 and 1, respectively), while \textcolor{black}{black} points are from the target domain.}
\end{center}

\end{frame}

% \begin{frame}{Results on the inter-twinning moons problem}
% \centering

% % --- First Row: Standard NN ---
% \begin{figure}
% \centering
% \begin{subfigure}[t]{0.30\linewidth}
%     \includegraphics[width=\linewidth]{label_decision_vanilla.png}
%     \caption*{\small Label Classification}
% \end{subfigure}
% \hfill
% \begin{subfigure}[t]{0.30\linewidth}
%     \includegraphics[width=\linewidth]{feature_vanilla.png}
%     \caption*{\small Representation PCA}
% \end{subfigure}
% \hfill
% \begin{subfigure}[t]{0.30\linewidth}
%     \includegraphics[width=\linewidth]{domain_deciison_vanilla.png}
%     \caption*{\small Domain Classification}
% \end{subfigure}
% \caption{(a) Standard NN}. 
% \end{figure}

% %\vspace{0.8cm}

% % --- Second Row: DANN ---
% \begin{figure}
% \centering
% \begin{subfigure}[t]{0.30\linewidth}
%     \includegraphics[width=\linewidth]{label_decision_dann.png}
%     \caption*{\small Label Classification}
% \end{subfigure}
% \hfill
% \begin{subfigure}[t]{0.30\linewidth}
%     \includegraphics[width=\linewidth]{feature_dann.png}
%     \caption*{\small Representation PCA}
% \end{subfigure}
% \hfill
% \begin{subfigure}[t]{0.30\linewidth}
%     \includegraphics[width=\linewidth]{domain_decision_dann.png}
%     \caption*{\small Domain Classification}
% \end{subfigure}
% \caption{(b) DANN (Algorithm 1)}
% \end{figure}

% \end{frame}


\begin{frame}{Results on the inter-twinning moons problem}
\footnotesize
\centering

% --------- First Row: Standard NN -----------
\begin{minipage}{0.8\linewidth}
\centering
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{ccc}
\includegraphics[width=0.3\textwidth]{label_decision_vanilla.png} &
\includegraphics[width=0.3\textwidth]{feature_vanilla.png} &
\includegraphics[width=0.3\textwidth]{domain_deciison_vanilla.png} \\
\small Label Classification & \small Representation PCA & \small Domain Classification
\end{tabular}
\end{adjustbox}
\end{minipage}

\vspace{0.0 cm}
\centering
\small\textbf{(a)} Standard NN.

\vspace{0.8cm}

% --------- Second Row: DANN -----------
\begin{minipage}{0.8\linewidth}
\centering
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{ccc}
\includegraphics[width=0.3\textwidth]{label_decision_dann.png} &
\includegraphics[width=0.3\textwidth]{feature_dann.png} &
\includegraphics[width=0.3\textwidth]{domain_decision_dann.png} \\
\small Label Classification & \small Representation PCA & \small Domain Classification
\end{tabular}
\end{adjustbox}
\end{minipage}

\vspace{0.3cm}
\centering
\small\textbf{(b)} DANN

\end{frame}

% \begin{frame}{\centering \large MNIST $\rightarrow$ MNIST-M: Top Feature Extractor Layer}
% \centering

% \begin{adjustbox}{max width=\textwidth}
% \begin{tabular}{cc}
% % Left Image
% \begin{subfigure}[t]{0.45\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{Source-only.png}
%     \caption{Non-adapted}
% \end{subfigure}
% &
% % Right Image
% \begin{subfigure}[t]{0.45\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{DANN.png}
%     \caption{Adapted}
% \end{subfigure}
% \end{tabular}
% \end{adjustbox}

% \vspace{0.3cm}
% \small
% \textbf{Figure:} t-SNE visualizations of extracted features. \textcolor{blue}{Blue} points: source domain (MNIST); \textcolor{red}{Red} points: target domain (MNIST-M). Adaptation aligns distributions better.

% \end{frame}

\begin{frame}{Domain Adaptation on Amazon review Datasets}
    \begin{table}[h!]
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
    \hline
    Source & Target & DANN & NN & SVM & DANN(p) & NN(p) & SVM(p) \\ \hline
    books & dvd & \textbf{72.8}\% & \textbf{72.8}\% & 71.95\% & 78.4\% & 79.0\% & \textbf{79.9}\% \\ \hline
    books & electronics & \textbf{70.9}\% & 69.5\% & 69.5\% & 73.3\% & 74.7\% & \textbf{74.8}\% \\ \hline
    books & kitchen & \textbf{71.85}\% & \textbf{71.85}\% & 71.7\% & \textbf{77.9}\% & 77.8\% & 76.9\% \\ \hline
    dvd & books & \textbf{71.55}\% & 66.9\% & 69\% & 72.3\% & 72.0\% & \textbf{74.3}\% \\ \hline
    dvd & electronics & \textbf{69.8}\% & 69\% & 66.63\% & \textbf{75.4}\% & 73.2\% & 74.8\% \\ \hline
    dvd & kitchen & \textbf{70.05}\% & 67.25\% & 69.5\% & \textbf{78.3}\% & 77.8\% & 74.6\% \\ \hline
    electronics & books & \textbf{65.95}\% & 63.8\% & 65.45\% & \textbf{71.3}\% & 70.9\% & 70.5\% \\ \hline
    electronics & dvd & \textbf{68.05}\% & 67.15\% & 67\% & \textbf{73.8}\% & 73.3\% & 72.6\% \\ \hline
    electronics & kitchen & \textbf{79.25}\% & 78.45\% & 78.75\% & \textbf{85.4}\% & \textbf{85.4}\% & 84.7\% \\ \hline
    kitchen & book & 68.25\% & \textbf{68.75}\% & 68.05\% & \textbf{70.9}\% & 70.8\% & 70.7\% \\ \hline
    kitchen & dvd & \textbf{68.90}\% & 65.15\% & 68.5\% & \textbf{74.0}\% & 73.9\% & 73.6\% \\ \hline
    kitchen & electronics & 78.75\% & 77.4\% & \textbf{79.25}\% & \textbf{84.3}\% & 84.1\% & 84.2\% \\ \hline
    \end{tabular}
    \end{adjustbox}
    \caption{Results of DANN, NN, SVM on Amazon Review Dataset compared with paper results.}
    \label{tab:sample_table}
    \end{table}
        

\end{frame}



\begin{frame}{\centering \large MNIST $\rightarrow$ MNIST-M: Top Feature Extractor Layer}
\centering
\begin{tabular}{cc}
% Left Image
\begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{Source-only.png}
    \captionof{figure}{Without DANN}
\end{minipage}
&
% Right Image
\begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{DANN.png}
    \captionof{figure}{With DANN}
\end{minipage}
\end{tabular}

\vspace{0.3cm}

\small
\textbf{Figure:} t-SNE visualizations of extracted features. \textcolor{blue}{Blue} points: source domain (MNIST); \textcolor{red}{Red} points: target domain (MNIST-M). Adaptation aligns distributions better.
\end{frame}

\section{Survey of Other Methods of UDA}

\begin{frame}
    \frametitle{Domain Invariant Feature Learning}
    \textbf{Key Idea:} The goal of domain invariant feature learning is to learn a representation that is invariant to the domain shift. Basically, aligning source and target domains by creating a domain invariant feature representation which follows the same distribution regardless of source and target.
 
    \textbf{Algorithms Explored:}
    \begin{itemize}
    \item CORAL(Correlation Alignment) and DeepCORAL
    \item MMD(Maximum Mean Discrepancy)
    \item DSN(Domain Separation Networks)
    \end{itemize}
\end{frame}

\subsection{CORAL and DeepCORAL}
 
\begin{frame}
\frametitle{CORAL}
    \textbf{Key Idea:} CORAL aligns the second-order statistics of the source and target distributions. It minimizes the distance between the covariance matrices of the source and target features. This is done by transforming the source data to a new feature representation by finding a linear transformation $A$($\mathcal{D}_s \rightarrow \mathcal{D}_S A$).\\
 
    \textbf{Mathematical Definition:}
    \[
        \mathcal{L}_{CORAL} = ||\widehat{C_S} - C_T||_F^2
    \]
    Where \(\widehat{C_S}\) and \(C_T\) are the covariance matrices of the transformed source($\mathcal{D}_SA$) and target($D_T$) features. We find an $A$ which minimises this CORAL loss.
\end{frame}
 
\begin{frame}
    \frametitle{CORAL: Experiments}
        Testing the CORAL method for basic robustness on randomly synthesised linearly separable data random as well as Gaussian datasets. We checked the accuracy for source and target domains for 2 scenerios in both the cases- same covariances and different covariances.\\
 
        \begin{table}
        \begin{tabular}{ccc}
            \toprule
            \textbf{Dataset Type} & \textbf{Covariance Condition} & \textbf{Accuracy (\%)} \\
            \midrule
            Univariate Random & Same Covariance & 100.0 \\
            Univariate Random & Different Covariance & 100.0 \\
            Multivariate Normal & Same Covariance & 95.80 \\
            Multivariate Normal & Different Covariance & 87.39 \\
            \bottomrule
          \end{tabular}
        \end{table}
        The accuracies are averaged over 100 iterations of data generation, model training, and testing in each case in the table.
    \end{frame}
 
\begin{frame}
    \frametitle{DeepCORAL}
    \textbf{Key Idea:} DeepCORAL extends the CORAL method to deep neural networks. It uses a deep network to learn a feature representation that minimizes a linear combination of the CORAL loss and the classifier loss.
 
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.9\linewidth, height=0.3\textwidth]{images/DeepCORAL/DeepCORAL.png}
        \caption{ Sample architecture on a CNN(AlexNet) with a classifier layer.}
    \end{figure}
    \begin{itemize}
    \item $\mathcal{L}_{CORAL} = \frac{1}{4d^2}||\widehat{C_S} - C_T||_F^2$
    \item Total Loss = $\mathcal{L}_{CORAL} + \sum_i\lambda_i\mathcal{L}_{CORAL}$
    \end{itemize}
\end{frame}
 
\begin{frame}
    \frametitle{DeepCORAL: Experiments}
    \textbf{Dataset:} Office dataset: Amazon, DSLR, Webcam. A computer vision dataset capturing pictures of office objects under different conditions.
    \begin{table}
        \caption{Classification accuracy (source $\rightarrow$ target) of DeepCORAL on the Office computer vision dataset. A: Amazon, D: DSLR, W: Webcam.}
        \label{comparePerformance2}
        \begin{scriptsize}
        \begin{center}
        {\renewcommand{\arraystretch}{1.4}
        \begin{tabular}{@{}l cccccc@{}}
        \toprule
        \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Result Source}}} & \multicolumn{6}{c}{\textbf{Office (Amazon, DSLR, Webcam)}} \\
        \cmidrule{2-7}
         & \textbf{A $\rightarrow$ W} & \textbf{D $\rightarrow$ W} & \textbf{W $\rightarrow$ D} & \textbf{A $\rightarrow$ D} & \textbf{D $\rightarrow$ A} & \textbf{W $\rightarrow$ A} \\
        \midrule
        \textbf{Our Code} & 62.05 & 95.32 & 99.56 & 64.44 & 52.77 & 56.49\\
        \hline
        \textbf{Survey Paper} & 66.4$\pm$0.4 & 95.7$\pm$0.3 & 99.2$\pm$0.1 & 66.8$\pm$0.6 & 52.8$\pm$0.2 & 51.5$\pm$0.3\\
        \bottomrule
        \end{tabular}
        }
        \end{center}
        \end{scriptsize}
    \end{table}
    We have compared the accuracies obtained in our experiments with those
    mentioned in the survey paper.
\end{frame}

\subsection{Maximum Mean Discrepancy (MMD)}
\begin{frame}
    \frametitle{Maximum Mean Discrepancy}
    \textbf{Concept:} MMD is a statistical test to measure the distance between two distributions.\\
    \textbf{Key Idea:} MMD is defined as the squared distance between the mean embeddings of two distributions in a reproducing kernel Hilbert space (RKHS).\\
    \textbf{Mathematical Definition:}
    \[
        \text{MMD}^2(\mathcal{D}_S, \mathcal{D}_T) = \left\| \frac{1}{n} \sum_{i=1}^{n} \phi(x_i) - \frac{1}{m} \sum_{j=1}^{m} \phi(y_j) \right\|_{\mathcal{H}}^2
    \]
    \[
        \text{MMD}^2_k(P,Q) := \mathbb{E}_{x,x'}[k(x,x')] + \mathbb{E}_{y,y'}[k(y,y')] - 2\mathbb{E}_{x,y}[k(x,y)]
    \]   
\end{frame}

\begin{frame}
    \frametitle{Estimation of MMD}
    MMD can be estimated using the empirical distributions of the samples. The empirical MMD is given by:
    \[
         X = \{x_1, x_2, \ldots, x_m\} \text{ and } Y = \{y_1, y_2, \ldots, y_m\}
    \]
    \scriptsize \[
        \widehat{\text{MMD}}^2(P, Q) = \frac{1}{m(m-1)} \sum_{i=1}^{m} \sum_{j \neq i}^{m} k(x_i, x_j) + \frac{1}{n(n-1)} \sum_{i=1}^{n} \sum_{j \neq i}^{n} k(y_i, y_j) - \frac{2}{mn} \sum_{i=1}^{m} \sum_{j=1}^{n} k(x_i, y_j)
    \]
    \normalsize
    \textbf{Key Idea:} The empirical MMD is an unbiased estimator of the true MMD.\\
    From this we can define a test statistic:
    \[
        T = \frac{\hat{\text{MMD}}^2(X,Y)}{\sqrt{\hat{V_m}(X,Y)}}
    \]
    Where \(V_m\) is unbiased estimator of the variance of the MMD.\\
    \textbf{Key Idea:} If \(T\) is large, then the two distributions are likely different.\\

\end{frame}

\begin{frame}
    \frametitle{Experiments on MMD}
    We have used synthetic data to test MMD. The experiment uses two different distributions:
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.9\linewidth]{mmdDataset.png}
        \caption{Synthetic datasets used for MMD experiments.}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Results of MMD}
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.8\linewidth]{Test_powe_vs_eps.png}
        \caption{Test power vs $\epsilon$ (epsilon). As the difference between distributions increases, MMD becomes more effective at distinguishing them.}
    \end{figure}
\end{frame}

% \begin{frame}
%     \frametitle{Results of MMD}

%     \begin{figure}[h]
%         \centering
%         \begin{minipage}{0.48\textwidth}
%             \includegraphics[width=\linewidth]{t_vs_sigma.png}
%             \caption{Test statistic T vs kernel bandwidth $\sigma$}
%         \end{minipage}
%         \hfill
%         \begin{minipage}{0.48\textwidth}
%             \includegraphics[width=\linewidth]{MMD vs sigma.png}
%             \caption{MMD value vs kernel bandwidth $\sigma$}
%         \end{minipage}
%     \end{figure}
% \end{frame}

\begin{frame}
    \frametitle{MMD in UDA}
    \textbf{Key Idea:} MMD can be used as a loss function to minimize the distance between the source and target distributions.\\

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.8\linewidth]{mmd_adaptation.png}
        \caption{MMD used in domain adaptation to align feature distributions.}
    \end{figure}

\end{frame}



% \begin{frame}
%     \frametitle{Ensemble Methods}

%     \textbf{Motivation:} Motivation of an Ensemble method is to use multiple diffrent models and then combine them to get a better performance. 
%     We belive that mistakes from different averages out giving an overall better performance. \\

%     \textbf{Methods used:} \\
%     For Combining the results from the different models we have used the following methods: 
%     \begin{itemize}
%         \item \textbf{Majority Voting [Classification]:} The class with the most votes is selected as the final prediction.
%         \item \textbf{Average [Regression]:} The average of the predictions is taken as the final prediction.
%     \end{itemize}
%     To obtain independent models we can do the following:
%     \begin{itemize}
%         \item Use different architectures for each model.
%         \item Use different initializations for each model.
%         \item Save instances of models on regular intervals while training.
%     \end{itemize}

    

% \end{frame}


\subsection{Domain Separation Network}
\begin{frame}
    \frametitle{Domain Separation Network}
    \textbf{Motivation:} Obtaining large datasets is very difficult and expensive. So we wish to use the data from a synthetically generated source and then adapt it to the real world.\\
    \textbf{Idea:} In this method we learn two types of features:
    \begin{itemize}
        \item \textbf{Domain Invariant Features:} These features are common to both the source and target domains. They are used to classify the data.
        \item \textbf{Domain Specific Features:} These features are specific to the source or target domain. They are used to separate the two domains.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{DSN Architecture}

    \begin{figure}
        \centering
        \includegraphics[width=0.9\linewidth]{DSN_Architecture.png}
        \caption{Domain Separation Network Architecture. The network learns both domain-invariant and domain-specific features through separate encoders.}
    \end{figure}

\end{frame}

% \begin{frame}
%     \frametitle{DSN Loss Function}
%     \[
%         \mathcal{L} = \mathcal{L}_{task} + \alpha \mathcal{L}_{recon} + \beta \mathcal{L}_{difference} + \gamma \mathcal{L}_{similarity}
%     \]
%     \[
%         \mathcal{L}_{task} = sum_{i=1}^{n} \mathbf{y_i}^s \cdot log(\mathbf{y_i}^s)
%     \]
%     \[
%         \mathcal{L}_{recon} = sum_{i=1}^{N_s}\mathcal{L}_{si_mse}(\mathbf{x_i}^s,   \mathbf{x_i}^s) + sum_{i=1}^{N_t}\mathcal{L}_{si_mse}(\mathbf{y_i}^t, \mathbf{y_i}^s)
%     \]


    

% \end{frame}

\begin{frame}
    \frametitle{Experiments on DSN}
    \textbf{Dataset:} We used the MNIST and MNIST-M datasets for the experiments.\\

    \begin{table}[h]
        \centering
        \begin{tabular}{lc}
        \toprule
        \textbf{Method} & \textbf{Accuracy} \\
        \midrule
        DSN (Ours) & 81.6\% \\
        DSN (Paper) & 83.2\% \\
        \bottomrule
      \end{tabular}
      \caption{Results comparing our DSN implementation with original paper}
    \end{table}


    

\end{frame}

\begin{frame}
    \frametitle{Results of DSN}

    \begin{figure}
        \centering
        % First row
        \begin{subfigure}[b]{0.3\linewidth}
            \includegraphics[width=\linewidth]{source.png}
            \caption{Source Images}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.3\linewidth}
            \includegraphics[width=\linewidth]{target.png}
            \caption{Target Images}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.3\linewidth}
            \includegraphics[width=\linewidth]{DNS_MNIST_MNISTM.png}
            \caption{Classification Results}
        \end{subfigure}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Results of DSN}

    \begin{figure}
        \centering
        \begin{subfigure}[b]{0.4\linewidth}
            \includegraphics[width=\linewidth]{reconstructed_source.png}
            \caption{\small Reconstructed Source Images}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.4\linewidth}
            \includegraphics[width=\linewidth]{reconstructed_target.png}
            \caption{\small Reconstructed Target Images}
        \end{subfigure}
        \caption{\small Experiments on DSN with MNIST and MNIST-M datasets}
    \end{figure}

    

\end{frame}


\subsection{Asymmetric Tri-training}
\begin{frame}
    \frametitle{Asymmetric Tri-training}

    Ensemble model approach to UDA. Utilizing a feature extractor \(F\) and three classifiers \(F_1,F_2,F_t\).\\
    \textbf{Key Idea:} Use two classifiers to pseudo-label the target domain and train a third classifier on the pseudo-labeled data.\\


    \begin{figure}[h]
        \centering
        \includegraphics[width=0.9\linewidth]{ATT_achi.png}
        \caption{Asymmetric Tri-training Network Architecture}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{MNIST - SVHN Domain Adaptation}
    \textbf{Dataset:} MNIST and SVHN are two datasets used for image classification. MNIST contains handwritten digits, while SVHN contains street view house numbers.\\

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.9\linewidth]{att-msvhn-a.png}
        \caption{The architecture used for training SVHN}
    \end{figure}

\end{frame}

\begin{frame}{Algorithm Overview}
    \begin{columns}
      % Left column for text
      \begin{column}{0.5\textwidth}
        \small % adjust text size if needed
        \textbf{Key Points:}
        \begin{itemize}
          \item \textbf{Loss Function for F1 and F2:}
          \tiny \[
            \begin{aligned}
            \mathcal{L}_{\Theta_F,\Theta_{F1},\Theta_{F2}} &= \frac{1}{n} \sum_{i=1}^{n} [L_y(F_1(F(x_i)); y_i)\\ &+ L_y(F_2(F(x_i)); y_i)] \\
            &+ \lambda |W_1^T W_2|
            \end{aligned}
          \]
          \item \normalsize \textbf{Loss Function for Ft:}
          \tiny \[
             \mathcal{L}(\theta_F, \theta_{F_t}) =
            \mathcal{L}_{CE}(F_t(x_t), \hat{y}_t) 
          \]
        \end{itemize}
      \end{column}
    
      % Right column for the image
      \begin{column}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{algo_att.png}
      \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Results of ATT on MNIST and SVHN Domain Adaptation}

    \begin{table}[h]
        \centering
        \caption{Results of ATT on MNIST and SVHN Domain Adaptation.}
        \label{tab:att_results}
        \begin{tabular}{lccc}
            \toprule
            \textbf{Method} & \textbf{MNIST\(\to\)SVHN} & \textbf{SVHN \(\to\) MNIST} \\
            \midrule
            Our w/o Batch Normalization & 36.9\% & 76.8\% \\
            Ours w/o Multi-view Loss & 15.2\% & 76.5\% \\
            Ours  & 15.2\% & 71.4\% \\
            \midrule
            Papers w/o Batch Normalization & 39.8\% & 79.8\% \\
            Papers w/o Multi-view Loss & 49.7\% & 86.0\% \\
            Papers  & 52.8\% & 85.8\% \\
            \bottomrule
        \end{tabular}
      \end{table}
\end{frame}


\begin{frame}{Contributions}
\textbf{Pratham Gupta:}
\begin{itemize}
    \item Codes for MMD and ATT algorithms
    \item Complete Main Section of Report
    \item MMD and ATT in Appendix Section of Report
    \item Presentation Slides of Introduction, DSN, MMD and ATT
    \item Video Editing
    \item Maintained Repository of the project
    \item Worked on DANN Re-identification Code
\end{itemize}

\vspace{0.5em}
\textbf{Gavish Bansal:}
\begin{itemize}
    \item Codes for Sentiment Analysis and Re-identification Code
    \item Complete Appendix Section of DANN and Theory paper in Report
    \item Prepared Presentation for DANN part
\end{itemize}
\end{frame}

\begin{frame}{Contributions (contd.)}
\textbf{Kintan Saha:}
\begin{itemize}
    \item Code for the inter-twined moons setup
    \item Code for image classification experiments of the DANN paper
    \item Writing the Appendix section for the DANN paper and the theory paper
    \item Helping in preparing the presentation slides for the DANN part
\end{itemize}

\vspace{0.5em}
\textbf{Krishna Agarwal:}
\begin{itemize}
\item Codes for CORAL, DeepCORAL, and DSN
    \item Helped in writing sections of CORAL, DeepCORAL, and DSN in the main report
    \item Writing the Appendix section for the Survey paper
    \item Presentation slides for CORAL and DeepCORAL
\end{itemize}
\end{frame}

\begin{frame}[c]
\centering
\Huge{\textbf{Thank You!}}\\[1em]
\LARGE{Any Questions?}
\end{frame}

\end{document}